case_id,retrieval_model,qna_model,error
Case2,perplexity,chatgpt,"Error code: 400 - {'error': {'message': ""This model's maximum context length is 128000 tokens. However, your messages resulted in 170264 tokens. Please reduce the length of the messages."", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
Case5,perplexity,chatgpt,"Error code: 400 - {'error': {'message': ""This model's maximum context length is 128000 tokens. However, your messages resulted in 170258 tokens. Please reduce the length of the messages."", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
Case20,perplexity,chatgpt,"Error code: 400 - {'error': {'message': ""This model's maximum context length is 128000 tokens. However, your messages resulted in 182360 tokens. Please reduce the length of the messages."", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
Case28,perplexity,chatgpt,"Error code: 400 - {'error': {'message': ""This model's maximum context length is 128000 tokens. However, your messages resulted in 182339 tokens. Please reduce the length of the messages."", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
Case44,perplexity,chatgpt,"Error code: 400 - {'error': {'message': ""This model's maximum context length is 128000 tokens. However, your messages resulted in 182350 tokens. Please reduce the length of the messages."", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
Case47,perplexity,chatgpt,"Error code: 400 - {'error': {'message': ""This model's maximum context length is 128000 tokens. However, your messages resulted in 182358 tokens. Please reduce the length of the messages."", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
